% Template for PLoS
% Version 3.6 Aug 2022
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended 
% to minimize problems and delays during our production 
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % % 
%
% Once your paper is accepted for publication, 
% PLEASE REMOVE ALL TRACKED CHANGES in this file 
% and leave only the final text of your manuscript. 
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file. 
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission. 
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.  For example, x$^2$ is incorrect; this should be formatted as $x^2$ (or $\mathrm{x}^2$ if the romanized font is desired).
%
% Do not include text that is not math in the math environment. For example, CO2 should be written as CO\textsubscript{2} instead of CO$_2$.
%
% Please add line breaks to long display equations when possible in order to fit size of the column. 
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.  For example, change "[U(D,E,\gamma)]^2" to "{[U(D,E,\gamma)]}^2". 
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % % 
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage[nopatch=eqnum]{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

\usepackage{pslatex}
%\usepackage{apacite}

\usepackage{cancel}
% The color of hyperlinks (URLs)
%\usepackage[backend=bibtex,style=nature,natbib=true]{biblatex} 
%\usepackage{biblatex} 
%\usepackage{natbib}


% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace} 
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
%\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
%\makeatletter
%\renewcommand{\@biblabel}[1]{\quad#1.}
%\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

%% END MACROS SECTION

% Some field suppression via options
\ExecuteBibliographyOptions{isbn=false,url=false,doi=false,eprint=false}

% One-paragraph bibliography environment
\defbibenvironment{bibliography}
  {\list
     {\printtext[labelnumberwidth]{%
        \printfield{prefixnumber}%
        \printfield{labelnumber}}%
      \ifentrytype{article}{% Suppress remaining fields/names/lists here
        \clearfield{title}}{}}
     {\setlength{\leftmargin}{0pt}%
      \setlength{\topsep}{0pt}}%
      \renewcommand*{\makelabel}[1]{##1}}
  {\endlist}
  {\mkbibitem}

% \mkbibitem just prints item label and non-breakable space
\makeatletter
\newcommand{\mkbibitem}{\@itemlabel\addnbspace}
\makeatother

% Add breakable space between bibliography items
\renewcommand*{\finentrypunct}{\addperiod\space}

% et al. string upright (nature style applies \mkbibemph)
\renewbibmacro*{name:andothers}{%
  \ifboolexpr{
    test {\ifnumequal{\value{listcount}}{\value{liststop}}}
    and
    test \ifmorenames
  }
    {\ifnumgreater{\value{liststop}}{1}{\finalandcomma}{}%
     \andothersdelim
     \bibstring{andothers}}
    {}}

\addbibresource{biblio.bib} % The filename of the bibliography
\usepackage{color}




\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Balancing accuracy and diversity : principles of model-driven active sampling in the brain} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Name1 Surname\textsuperscript{1,2\Yinyang},
Name2 Surname\textsuperscript{2\Yinyang},
Name3 Surname\textsuperscript{2,3\textcurrency},
Name4 Surname\textsuperscript{2},
Name5 Surname\textsuperscript{2\ddag},
Name6 Surname\textsuperscript{2\ddag},
Name7 Surname\textsuperscript{1,2,3*},
with the Lorem Ipsum Consortium\textsuperscript{\textpilcrow}
\\
\bigskip
\textbf{1} Affiliation Dept/Program/Center, Institution Name, City, State, Country
\\
\textbf{2} Affiliation Dept/Program/Center, Institution Name, City, State, Country
\\
\textbf{3} Affiliation Dept/Program/Center, Institution Name, City, State, Country
\\
\bigskip

% \author{\large \bf Anonymous authors}
%\author{{\large \bf Hamza Oueld (hamza.oueld-kaddour-el-hallaloui@univ-amu.fr)}$^{1}$ \\
%\AND {\large \bf Andrea Brovelli (andrea.brovelli@univ-amu.fr)}$^{1}$ \\
%\AND {\large \bf Emmanuel Dauc√© (emmanuel.dauce@univ-amu.fr)}$^{1,2}$ \\
%1.Institut de Neurosciences de la Timone, Aix-Marseille Univerity, 
%Marseille, France\\
%2.Ecole Centrale de Marseille,
%Marseille, France
%}

% Insert additional author notes using the symbols described below. Insert symbol callouts after author names as necessary.
% 
% Remove or comment out the author notes below if they aren't used.
%
% Primary Equal Contribution Note
\Yinyang These authors contributed equally to this work.

% Additional Equal Contribution Note
% Also use this double-dagger symbol for special authorship notes, such as senior authorship.
\ddag These authors also contributed equally to this work.

% Current address notes
\textcurrency Current Address: Dept/Program/Center, Institution Name, City, State, Country % change symbol to "\textcurrency a" if more than one current address note
% \textcurrency b Insert second current address 
% \textcurrency c Insert third current address

% Deceased author note
\dag Deceased

% Group/Consortium Author Note
\textpilcrow Membership list can be found in the Acknowledgments section.

% Use the asterisk to denote corresponding authorship and provide email address in note below.
* correspondingauthor@institute.edu

\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
Understanding our environment requires not only passively observing sensory samples but also \emph{acting} to seek out useful relationships between our actions and their possible outcomes. Inspired by the principle of visual salience, we introduce the concept of an "ideal participant", which is the active counterpart of a Bayesian ideal observer. During learning, an ideal participant would build a statistical model of its environment while updating its policy (action selection) after each new observation. This action selection requires a tight balance between making correct predictions (accuracy) and providing diverse data to feed the models (diversity).
We assess these principles in a knowledge-oriented action selection task called the "volleyball" task, where participants estimate the causal influence of a player on the outcome of a volleyball game. The behavioral data we have collected suggest that such active sampling strategies do occur in the brain and improve the accuracy of action/outcome models. We show that the balance between accuracy and diversity objectives can lead to specific action selection biases, reflected both in the model and in the experiments.

% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step. 
% Author Summary not valid for PLOS ONE submissions.   
\section*{Author summary}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur eget porta erat. Morbi consectetur est vel gravida pretium. Suspendisse ut dui eu ante cursus gravida non sed sem. Nullam sapien tellus, commodo id velit id, eleifend volutpat quam. Phasellus mauris velit, dapibus finibus elementum vel, pulvinar non tellus. Nunc pellentesque pretium diam, quis maximus dolor faucibus id. Nunc convallis sodales ante, ut ullamcorper est egestas vitae. Nam sit amet enim ultrices, ultrices elit pulvinar, volutpat risus.

\linenumbers








%    Significance

%commencer par le paradoxe des pigeons qui choisissent une action de mani√®re aleatoire (+ aller voir les refs Boraud). 
%Le comportement consiste alors a biaiser les reponses vers une r√©ponse particuli√®re, velle qui apporte le meilleur rendement. N√©anmoins, la strategie suivie par ls animaux est souvent loin de la strat√©gie optimale (thompson sampling).

%Une hypoth√®se a √©t√© √©mise comme quoi le cerveau construit des descriptions probabilistes des relations entre ses actions et les effets de ses actions. model-based vs model free action selection.  
%A striking property of the brain is its natural capability to infer statistical relationships between the observed variables of its environment, through a large variety of experimental setups [refs]. 

\section{Problem setting}


Our environment is full of unpredictable events, and the Bayesian brain hypothesis \cite{knill2004bayesian} suggests that our brains build models to better predict these events. This assumption relies on the idea that the sensory environment acts as a source of random events, against which the brain would construct probabilistic models that would enable it to better predict and anticipate these events \cite{friston2005theory}. This is also known as inference, in the sense that the brain would start from a certain number of hypotheses about the state of the environment, and use the available data to confirm or eliminate some of them. A fundamental feature of this inference process is its predictive nature, which means that the brain's predictions can be compared with the actual data \cite{rao1999predictive}. It is generally assumed that these predictions are obtained by sampling the probabilistic model, and that the validity of the model (or of the hypotheses) is compared with the accuracy of the predictions made \cite{griffiths2008bayesian}. The framework of model inference is considered as a plausible mechanism of learning in the brain, where updating the model according to the prediction error is the central operation, based on Bayes' rule \cite{doya2007bayesian}.

%Crucially, at no point does it "see" probabilities; it only sees realizations. 
%This is referred to as (Bayesian) inference. 

From the formal standpoint, learning the statistical structure of the environment is a typical estimation problem \cite{bishop2006pattern}. Its numeric implementation implies doing some prior assumptions on the statistical model to use, and then estimating its parameters under the Bayesian framework from observing samples \cite{gelman1995bayesian}. The parameters of the model play here the role of a hidden variable, that are \emph{inferred} from observing their outcome. In that framework, a typical objective is the maximization of the likelihood of the outcome, given the parameters (Maximum Likelihood Estimate) \cite{myung2003tutorial}, or the maximization of the Evidence Lower Bound (ELBO), a compound objective also considering minimizing the complexity of the model \cite{kingma2013auto,blei2017variational}.

This probabilistic approach has long been used as a plausible mechanism of visual perception, where the statistical distribution of natural images, under the constraint of parsimony, allows to explain the formation of orientation receptive fields in V1 \cite{simoncelli2001natural}, as well as certain "extra-classical" receptive fields \cite{rao1999predictive}. The visual system seems to operate similarly to an "ideal" Bayesian estimator \cite{geisler1989sequential,geisler2008visual}. It is also interesting to note that the same fundamental principles of estimating a latent distribution from a large number of examples is at the core of unsupervised learning, e.g. in variational auto-encoders and image generation \cite{hinton1995wake, kingma2013auto, rombach2022high}.


%Cependant, ce principe d'estimation ne semble pas se cantonner au simple syst√®me visuel et appara√Æt comme un m√©canisme tr√®s g√©n√©ral gr√¢ce auquel le cerveau adapte de mani√®re flexible ses choix et ses r√©ponses aux donn√©es de l'environnement \cite{friston2010free}. Si l'estimation de donn√©es et la pr√©diction semblent constituer le m√©canisme √† travers lequel sont trait√©es les donn√©es sensorielles \cite{knill2004bayesian}, il semble, de mani√®re plus fondamentale, que ce m√™me principe pourrait √©galement s'appliquer √† la s√©lection de l'action . Il existe en effet une correspondance classique entre la s√©lection de l'action et l'estimation, en particulier dans le cadre du contr√¥le optimal bas√© sur les mod√®les, pour lesquels le mod√®le de la commande s'appuie sur le mod√®le de l'environnement \cite{}.

However, this principle of estimation through a generative model does not seem to be confined to the visual system alone and appears as a general mechanism by which the brain flexibly adapts its choices and responses to environmental data . If data estimation and prediction seem to constitute the mechanism through which sensory data is processed, it seems, in a more fundamental way, that this same principle could also be applied to action selection \cite{friston2010free}. 

\begin{itemize}
\item On the one side, there is a classic correspondence between action selection and estimation, particularly in the context of model-based optimal control, where the control model rests on inverting the (forward) model of the environment \cite{todorov2001optimal,kording2004bayesian}. More indirectly, in the model-free context, learning the policy rests on selecting an action that will maximize the cumulative sum of rewards which, in the simple bandit case, resumes to maximizing the outcome itself \cite{sutton1998introduction}. Interestingly, a statistical Bayesian framework may also conduct to consider the policy as a parametric \emph{probability on action}, with maximizing the outcome interpreted as maximizing the (log)-likelihood of the policy (in a policy parametric space) [Levine].

%Then, predictive coding \cite{rao1999predictive} reflects the capacity of sampling those probabilities to predict what we'll sense. 
\item On the other side, as our body's movements affect what we sense, the active inference framework assumes that we  learn by making purposeful moves to understand our surroundings better \cite{friston2012perceptions} . %We choose our actions based on estimating information or prediction errors. 
Interestingly, in that framework, the goal is not only to minimize errors in our predictions, but also maximizing novelty or surprise, in order to seek out information, a concept known as Bayesian surprise \cite{ITTI20091295}. Maximizing novelty (that is current prediction error) is indeed important if one expect to better understand one's environment and avoid future misinterpretations. This search of novelty %as an interpretation of movement selection 
was first proposed by Itti and Baldi \cite{ITTI20091295}, telling that the eye is attracted by the part of the visual scene that is the most surprising regarding the current data model. Put in a Bayesian framework, this corresponds to maximizing model update, given the available data, measured here as a Kullback-Leibler divergence between the old and the new models, seen as a proxy for the prediction error \emph{reduction} after observing new data. This quantification of novelty appeared later on to have a pivotal role in the theory of active inference, where it is referred as the ``epistemic value'' of sensory data [REF]. Say in short, one should put attention to the prediction errors that significantly contribute to minimizing future prediction errors. 

%[LINK WITH INFORMATION THEORY]
% Tishby and Polani introduced the concept of empowerment within the framework of information theory as a measure of an agent's ability to influence its future state through its actions. Empowerment is quantified by the channel capacity between an agent's current actions and its future states, effectively measuring the entropy of the future states that the agent can achieve \cite{tishby2011information, polani2006formal}. This concept is closely related to the information bottleneck principle, which aims to find a compressed representation of an input variable that preserves the most relevant information about an output variable \cite{tishby2000information}. In the context of empowerment, the goal is to maximize the mutual information between actions and future states, akin to maximizing relevant information in the information bottleneck framework. By maximizing empowerment, an agent adopts behaviors that enhance its control and adaptability, making it more robust and flexible in dynamic environments \cite{klyubin2005empowerment}. These ideas have significant implications for developing autonomous systems in robotics and artificial intelligence, providing a theoretical basis for adaptive and resilient behavior.

%This principle can be generalized by assuming that our sensory environment

Moreover, the magnitude of the prediction error made by the model, in other words the "surprise", is related to the concept of entropy and information: data is more informative the more it deviates from the expected distribution. In a sensorimotor context, Tishby and Polani have shown that an agent has the capability to control information by maximizing the entropy of the distribution of observed states through its actions \cite{polani2014}. More precisely, they introduce the concept of ``Empowerment'', which refers to the capacity of the ``communication channel'' between the agent's actions and the observed states.
In the context of Empowerment, the goal is to maximize the mutual information between actions and future states, akin to maximizing relevant information in the information bottleneck framework \cite{tishby2000information}.
\end{itemize}

Importantly, the two principal objectives, i.e. either reaching nominal response or maximizing information, are not necessarily compliant, which is reminiscent of the  "exploration/exploitation trade-off", at the core of reinforcement learning \cite{sutton1998introduction}. However, quite surprisingly, no satisfactory solution has emerged to date to conciliate model learning (through information seeking) with optimal policy learning.
%[les developpements math√©matiques de ces id√©es conduisent √† une situation plutot confuse o√π de nombreux mod√®les concurrents de surprise sont employ√©s, souvent dans le cadre de l'optimisation motrice / s√©lection de l'action, sans qu'il soit √©vident de les d√©partager. La d√©marche adopt√©e dans cet article consiste √† analyser dans le d√©tail une situation de choix d'action motiv√©e par l'apprentissage d'une relation de cause √† effet entre les actions choisies par le sujet et les effets attendus. Il ne s'agit pas ici de maximiser une r√©compense future, mais plutot de d√©velopper la connaissance la plus pr√©cise possible du lien entre les choix faits et leurs cons√©quences observables, en pr√©sence de bruit. Dans ce cadre extr√™mement simplifi√©, nous verrons que l'utilisation d'un mod√®le hi√©rarchique permet de rendre compte des principales quantit√©s d'int√©r√™t √† l'oeuvre dans les mod√®les de s√©lection de l'action, et de les comparer, en tant que mod√®les de d√©cision, aux choix r√©ellement effectu√©s par des sujets humain. En particulier, l'utilisation  de donn√©es electrophysiologiques (MEG) permettent de mettre en √©vidence les r√©seaux c√©r√©braux √† l'oeuvre dans la prise en compte de ces diff√©rentes quantit√©s.]
More detrimentally, the developments of these ideas lead to a rather confusing situation where many competing models of surprise are employed (i.e. Bayesian surprise, Variational Free Energy, Empowerment etc.), either through motor optimization, action selection or in decision making, without it being obvious how to differentiate between them. {\color{magenta} Ironically, list of more than XX possible surprise metrics has even been proposed recently [ref gerstner]}. The approach adopted in this article involves a detailed analysis of a situation where action choice is motivated by learning a cause-effect relationship between the subject's chosen actions and the expected effects. This is not about maximizing future rewards but rather about developing the most precise knowledge possible of the link between choices made and their observable consequences, in the presence of noise. In this extremely simplified framework, we will see that using a hierarchical Bayesian model accounts for the main quantities of interest in action selection models and allows for comparison, as decision models, with the actual choices made by human subjects \cite{todorov2002optimal}. In particular, the use of electrophysiological data (MEG) highlights the brain networks involved in considering these different quantities \cite{brookes2011measuring}.

%It was proposed by Itti and Baldi  that the eye is attracted by the part of the visual scene that is the most surprising regarding the (sensory) data model. 
%This principle can be generalized by assuming that our sensory environment
%is mostly interpreted through acting (first) and (then) observing the sensory consequences of our actions. 
%This model-oriented action selection principle is consistent with a Bayesian observer that requires more data to construct his model. 
%More recently, it was proposed by  that the brain may act as a scientific investigator, that would select an action so as to validate (or dismiss) a current hypothesis about the state of the environment.  
%In that case, the difference between the model predictions and the actual sensory observations serves as a metric to interpret the current observation as more or less surprising, 
%and adapt either the model or the action selection so as to minimize the global surprise (that is the surprise over both past and future observations).

%    Originality

%Taking inspiration from Itti and Baldi saliency principle, the idea here is to select actions in a way that should maximize the knowledge about the data model. Let $\pi$ be the \emph{policy}, i.e. an adaptive action selection strategy, and let $\theta$ be the parameters of a data model, that reflects the distribution of expected sensory observations and outcomes observed after an action $a$. Let  $s \in \mathcal{S}$ be the state observed after an action $a$ is actuated.
%Then $p(s|a, \theta)$ is a parametric model of state transitions, that describes the expected distribution of outcomes given an action and the parameters. In a Bayesian setup, $\theta$ is interpreted as a latent variable that explains the current observation. Then, let $q(\theta)$ be a distribution over the latent parameters (posterior distribution) that is updated after each observation. In the simplest case, $p$ is a Bernouilli distribution and $q$ is a beta model over the the (single) parameter of $p$. 

% In model-based reinforcement learning, the statistical relationships between the actions and their outcomes inform the policy for the choice of the action. These statistical relationships can be given in advance, or learned from experiencing action/outcome samples. In the last case, the choice of action (policy) reflects a balance to be kept between two concurrent objectives, one objective being the building of an accurate model of the world, the second one being the maximization of the (cumulative) rewards. The difficulty in balancing exploration and exploitation, in the absence of a definite model, is at the heart of reinforcement learning problems. 



%On the other sie, l }

\subsubsection{Bayesian ideal observer}



A Bayesian ideal observer experiments the statistical structure of its environment from observing a series of samples, without intervening on it. It generally assumes a parametric model $\theta$ to explain the observations $y_1, y_2, ...$, with each $y_t$ assumed a sample of $p(Y|\theta)$, $\theta$ being unknown.

%It belongs to the family of variational inference algorithms. The model assumes that a . Here the set of parameters $\theta$ take the role of a hidden variable. 
At every step of the process, the agent forms an estimate of the parameters, $p(\Theta|h_t, y_t)$ with $h_t$ the sequence of past observations and $y_t$ the current observation. Then, the update of the model rests on applying Bayes formula on the new data using the chain rule: $p(\Theta|h_t, y_t) \propto p(y_t|\Theta) p(\Theta|h_{t-1}, y_{t-1}) $.
 
In the case of a Bernouilli series of inputs, the update of a simple Beta model $\theta \sim \text{Beta}(\alpha,\beta)$ (see previous sections) can be shown to follow that of a Bayesian ideal observer. 

It is to be noticed that, by construction, an observer does not intervene onto the data it observes. It is a pure passive method of estimation.    


%\subsubsection{Approximated active inference for Bernoulli bandit}

% ... (see previous version of the latex document if needed to retrieve info about Active Inference)

\subsection{Combining reward and information gain objectives}

{\color{blue}

\subsection{Controlling the data}

Hypothesis: Actions serve to sample sensory data.
Body movements (actions) are a source of sensory changes
Active learning: actions aim to better understand (infer) data.
Minimization of posterior prediction error (Active inference): via maximization of novelty (Bayesian surprise).

%The hypothesis that actions serve to sample sensory data posits that body movements are sources of sensory changes, facilitating active learning by enhancing data inference. According to this view, actions are not merely responses to external stimuli but are proactive processes aimed at gathering information about the environment. By actively engaging with their surroundings, organisms can improve their understanding and predictions of environmental states \citep{clark2013whatever}.
Active learning suggests that the purpose of actions is to minimize uncertainty by continuously updating internal models based on new sensory inputs. This process is akin to hypothesis testing, where the brain generates predictions and then tests these predictions against actual sensory data. Such an approach allows the brain to refine its models, making them more accurate and reliable over time. % \citep{sutton2018reinforcement}.
%The concept of active inference further elaborates on this hypothesis by proposing that actions are aimed at minimizing posterior prediction error. This is achieved through the maximization of novelty or Bayesian surprise, where the brain seeks out new and unexpected sensory data to reduce uncertainty in its predictive models. By continually seeking out novel experiences, the brain can challenge and update its models, ensuring they remain relevant and effective \citep{friston2010action}.
%This framework suggests that the brain operates in a constant loop of prediction and correction, where prediction errors signal the need for model adjustment. The minimization of prediction error through active inference is a core principle of this model, relying on the Bayesian framework to update beliefs and predictions \citep{rao1999predictive}. This dynamic interaction with the environment is essential for maintaining a high level of adaptability and for optimizing behavior in a constantly changing world \citep{friston2015active}.

In summary, the hypothesis that actions serve to sample sensory data underscores the importance of proactive engagement with the environment in sensory perception and cognitive function. Through active learning and the minimization of prediction errors, organisms can refine their internal models, better anticipate changes in their surroundings, and enhance their overall adaptability and survival.



On contrary to an observer, a reinforcement learning algorithm exerts a \emph{control} on the data it collects. Like in Thompson sampling, the observer may thus be an element of a larger estimation process that would take action over and causal relations within the data into consideration. 

Though generally assumed to maximize the reward expectation, the sampling of the data remains an important part of the process. It has been the subject of an abundant literature in the RL and robotics community, in order to go beyond the baseline uniform sampling on actions ($\varepsilon$-greedy). This comprise different variants of novelty detection, learning improvement and curiosity metrics [REFS]. Such exploration-incentive metrics remain however quite arbitrary and ``hand-made'', and remain difficult to incorporate into the RL optimization framework. 

Assuming $\theta$ a current statistical model explaining the past observations, we consider here  the process that selects new data in order to feed the model. In principle, one should choose a sample that would reduce the prediction error, i.e. increase the likelihood of the next observations after the model has been updated. 
%reduce the entropy of the posterior, expressed as a KL-divergence between posterior and the prior of the model's belief, a quantity known as the ``Bayesian surprise'' [REF]. Using Bayes formula, it closely corresponds to the . %Incidentally, the expected prediction error itself (before updating the model) is often used as a proxy for future model improvements, explaining the natural attraction toward random/unpredictable outcomes [REF].

In the simple scenario considered here, the outcome is assumed to be the result of a generative process combining the selection of the player $x_t$, and the generation of the outcome $y_t$, assuming the following mixture generative model $y_t \sim  p(Y_t|\Theta_t)$ with $\theta_t$ a hidden parameter that is conditionally dependent on $x_t$ (mixture variable) and the history of past observations $h_t$, i.e. $\theta_t \sim  q(\Theta|x_t, h_t)$. At each step $t$, let $q_{t|t-1}(\Theta)$ be the belief about $\theta_t$ before observing the data sample ($x_t$ and $y_t$) and $q_{t|t}(\Theta)$ be the belief about $\theta_t$ after observing the sample. Then maximizing the Information gain (or learning progress) is equal to maximizing the Kullback-Leibler divergence:
\begin{align}\label{eq:BS}
\text{max}_{x_t}\text{KL}(q_{t|t}(\Theta)||q_{t|t-1}(\Theta))
\end{align}
reflecting the model update (knowledge improvement) before and after observing the data.
%As time passes, the belief should converge to a point distribution and the Bayesian surprise should converge to zero.

This conducts to biasing the selection of action in favour of data that would maximises the update of the model [REF]. However, predicting the information gain is more complex than just predicting the average outcome. In a Thompson sampling setup, where each arm is associated with a Beta model, estimating the information gain for each arm requires (i) first sampling the current model, then (ii) generating an outcome for each arm by sampling the generative model, and finally (iii) estimating the belief update. [ALGORITHM?]. This approach is consistent with the predictive coding principle [REF], interpreted here as \emph{predicting the prediction error}.

Additionally, one can also provide a statistics over action selections from observing choices over time. This is the case, for instance, for a coach that would select (or not) a player for matches. Assume now that $\phi=(\pi, \theta)$ with $\pi$ a parameter for action selection (reflecting a proportion of action, i.e. ``PLAY'' or ``NOT PLAY'') and $\theta$ a parameter for predicting the outcome (i.e. ``WIN'' or ``LOSE''). Here $\pi$ would reflect the choices of the coach, and $q(\PI)$ (action selection statistics) could be updated at each step. %Knowing $\theta_x$ would help to provide a more comprehensive model of the way the observations are generated. 
Then, each observation of $y_t$, knowing $x_t$, would allow to improve the generative model by: \begin{align}\label{eq:BS-joint}
\text{max}_{x_t}\text{KL}(q_{t|t}(\Pi, \Theta)||q_{t|t-1}(\Pi, \Theta))
\end{align}
reflecting both the knowledge improvement about the coach's  behavior and match's outcome after observing the data sample.

Importantly here,  an additional element (reward information) comes into the play. In our case,  $y_t$ would indicate whether the choice was correct or incorrect (``WIN'' or ``LOSE''). %In a RL setup, this can be interpreted as an incentive to bias the coach's choice (i.e $\theta_x$) in favor of a policy parameter that would increase the probability of winning. 
Consider for instance $\theta$ being the ``WIN'' expectation of a Bernoulli distribution. Assuming $\pi$ being the ``PLAY'' expectation, then taking $\pi \propto\exp(\theta)$ would be consistent with biasing the selection of action toward maximizing the ``WIN'' expectation [ref Levine]. More generally, %assuming such strategy should be adopted by the coach, 
this correspondence may be given by $q(\Pi|\Theta)$, i.e. a belief about $\pi$ (action selection) given the belief about $\theta$ (the outcome i.e. the action validity). Injecting this relation into equation (\ref{eq:BS-joint}) may thus allows to incorporate reward information into the information gain maximization. This would conduct to a concurrent (or dual) optimization, reflecting both knowledge and action validity improvement (see also \cite{dauce2022concurrent}).
%Then assuming $q_{t|t-1}(\Pi)$ the current belief (obtained from observing the coach's choices), one should consider maximizing:
%\begin{align}\label{eq:BS-cond}\text{max}_{x_t}\text{KL}(q_{t|t}(\Pi|\Theta)||q_{t|t-1}(\Pi)) 
%\end{align}
%the information gain about the behavior, reflecting the (expected) behavior improvement expected from considering outcomes. 

%The complementary conditional distribution is $q(\Theta|Q)$ (a belief about $\theta$ given a belief about $Q$).
%In a predictive setup, one can now predict the next update of $\Theta$ knowing the current belief on $Q$. This is noted $q_t(\Theta,Q) = q_t(\Theta|Q)q(Q) $ = a winning probability, then one can assume . Assuming 
%$$\text{KL}(q_{t}(Q|\Theta)||q_{t-1}(Q)) + \text{KL}(q_{t}(\Theta)||q_{t-1}(\Theta|Q))$$
%reflecting the separation of the optimization in two terms. The left one reflecting the update of $Q$ after observing $x_t$ and $y_t$, and the right being the data model improvement, knowing $Q$.

%The information gain can be rewritten as: 
%with the left term reflecting the ``matching'' of the coach's choice with the proportion of ``WIN'', and more generally the matching of the behavior with rewards. The putative mismatch between $Q$ and $\Theta$ development of the formula, given in the algorithm, conducts to 


} 

%

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figs/vb_task.png}
\caption{Participants in the "Volleyball" task act as trainers tasked with hiring players for different teams. They can simulate the outcome of 40 matches with or without a particular player. They have to decide before each match whether to include the player or not ("PLAY" or "NOT PLAY"). Then, they observe the match outcome, and repeat the process until all 40 trials are completed. They are finally asked to assess the player effect on the team's performance.}
\label{fig:1}
\end{figure}

%We propose in this paper to extend this principle toward the case of model building in goal-directed action selection tasks: what is the appropriate strategy to choose when the objective is to understand the relationship between different actions and an their outcomes? 
%We consider here a probabilistic setting, where learning is reflected in estimating a conditional distribution $p(S|A)$ (where the outcome $s \in S$ conditionally depends on the action $a \in A$). A typical strategy is trying different actions, observing their outcomes and memorizing the average outcome for each action.  

%We want to understand whether the behavior adopted in information-seeking situations resembles that of saliency-based saccade selection in vision. In vision, the region of interest is the one whose statistics differs the most from the average image statistics, reflecting the information contained in the different parts of an image. 
%I

Inspired by the Bayesian ideal observer \cite{geisler1989sequential}, we thus developed a model of optimal sampling that we call the ``ideal participant''. An ideal participant is defined as an action selection mechanism (an "actor") that is built on top of an ideal observer.
%Let ${D}_{t}=(a_1,s_1), ..., (a_t,s_t)$ be a sequence of observations; a Bayesian observer is a recursive parameter update strategy that relies on Bayes formula, i.e. $q(\theta|\mathcal{D}_n) \propto p(s_n|a_n, \theta) \times q(\theta|\mathcal{D}_{n-1})$. An important element is the initial \emph{prior} $q_0(\theta)$ reflecting the initial guess about the parameters. 
%In an active sampling strategy, one need to consider specific series of actions $(a_1,...,a_n,...)$, and estimate how the corresponding  observations participates in reducing the uncertainty of the observer. 
The role of the observer is to build a  model $\theta$, reflecting the statistics of the outcomes obtained under any action $a$. The role of the actor (participant) is to choose the next action. At each step, the observer informs the actor how good the action $a$ is at reducing the model uncertainty.  The ``value'' of an observation is thus reflected in the information it provides to the model. 
Given a current data model (provided by the ideal observer), the ideal participant can estimate how unexpected the outcome is regarding the model, i.e. $-\log p(s|a,\theta)$, which can be compared to the marginal surprise $-\log p(s|\theta)$. The difference $\log p(s|a,\theta) - \log p(s|\theta)$ is the \emph{Information Gain} provided by the action $"a"$.
Maximizing the information gain turns out, in average,  to
maximizing the Empowerment \cite{Klyubin2005EmpowermentAU}, which is the channel capacity of an action-outcome medium. The concept is different from that of Bayesian surprise \cite{ITTI20091295}, used in classic (Free Energy based) active inference \cite{friston2012perceptions}.

%\begin{equation}
%    \mathcal{E} = \max_\pi \mathbb{E}_{s\sim \pi(A)} \mathbb{E}_{s\sim p(S|a, \theta)} \log p(s|a,\theta) - \log p(s|\theta, \pi) \label{eq:emp}
%\end{equation}
%This conducts to treat $\log p(s|a,\theta) - \log p(s|\theta,\pi)$ as a reward, 
%which corresponds, for a given data model $\theta$, to choose an action that should maximize the difference between the surprise and the \emph{marginal surprise}, with the marginal distribution of states defined as:
%$$ p(s|\theta,\pi) = \mathbb{E}_{a\sim\pi(a)} p(s|a,\theta)$$

%Id√©e : l'√©quation de l'empowerment repr√©sente un point d'√©quilbre entre deux tendances : l'aversion √† l'erreur (le mod√®le doit √™tre pr√©cis dans ses pr√©dictions), ce qui revient √† minimiser la surprise, mais √©galement une attirance pour la nouveaut√©, exprim√©e par le terme d'entropie de la distribution marginale. Cet √©quilibre entre deux tendances contradictoires   
Consider now that the objective of the observer (i.e. the brain) is to develop a good understanding (a good model) of its environment. Then, the selection of action will be based on comparing those two information quantities. The positive term  $\log p(s|a,\theta)$ reflects a \emph{precision} objective (that is having low prediction errors), while the negative term $- \log p(s|\theta)$ reflects a sampling diversity objective (state coverage). Only the combination of both may help to provide a comprehensive modeling of the environment. This objective can be attained at the cost of maintaining a general (marginal) model of the observations.   

\section{Experiments}
In order to illustrate the principles exposed, we consider in the following a behavioral task called the "volleyball" task (see figure \ref{fig:1})\cite{basanisi2021neurophysiological}, in which participants are asked to estimate the causal influence of a particular player in making the volley-ball team win or lose the matches. 15 different scenarios are considered. Each scenario is described by two Bernoulli distributions : $p_1=p(s=\text{WIN}|a=\text{PLAY})$ and  $p_0=p(s=\text{WIN}|a=\text{NOT PLAY})$ that reflect the probability to win when the player is selected versus non selected. In a first series of experiments (``experiment A'').  The influence of the player is quantified by the probability difference $\Delta p = p_1 - p_0$, that can vary from strong worsening ($\Delta p = -0.6$ winning probability) to strong improvement ($\Delta p = 0.6$ winning probability). In a second series of experiments (``experiment B''), the influence of the player can only vary from neutral (no change) to strong improvement ($+0.8$ winning probability).
By alternating 'PLAY' and 'NOT PLAY', the human participants obtains samples of 'WIN' and 'LOSE' and are expected to estimate both distributions (which is asked after 40 trials).

\section{Results}
%Ideal observer models  are used to estimate, in perception tasks, how subjects deviate from an optimal response, knowing the parameters of a task. 
We are interested here in the action selection strategies developed by the subjects, and by the action selection \emph{biases}. In such a setup it is easy to show that a close to optimal strategy is to choose the actions 'PLAY' and 'NOT PLAY' in equal proportion. A first and important observation is that human subjects exhibit mostly irregular action selection, resembling that of a Bernoulli draw, rather than a periodic alternation. %A second observation is that they manifest significant biases toward the actions that provide higher rewards (that is the "reward bias"), which reflects a compelling preference to 'WIN' rather than 'LOSE', which is largely expected in this kind of setup (not shown). 

More intriguingly, a significant bias toward entropic (action, outcome) relationships is observed in experiment A only (that is action-outcome relationships that are "difficult" to predict). In order to quantify this entropy bias, the entropy difference $\Delta H = H(p_1) - H(p_0)$, where $H(p)=-\sum_{s \in S}p(s)\log(p(s))$ is the entropy, indicates whether $p_1$ is more (or less) irregular than $p_0$. %The entropy difference bias can be seen as resulting from an imbalance between the precision and the diversity objectives, 
The question comes why such an imbalance is observed in the first experiment and not in the second? We thus simulated an ``ideal participant'' resolving similar tasks.
The ideal observer builds a posterior $q(\theta) = (\beta(n_{\text{WIN}}^{\text{PLAY}}, n_{\text{LOSE}}^{\text{PLAY}}), \beta(n_{\text{WIN}}^{\text{NOT PLAY}}, n_{\text{LOSE}}^{\text{NOT PLAY}}))$, with $\beta$ a beta model with $n_s^a$ being the number of $(a,s)$ observations. The marginal distribution $\beta(n_{\text{WIN}},n_{\text{LOSE}})$ is estimated in addition.
Then, the ideal participant samples $ \tilde{p}_0,  \tilde{p}_1$ from the ideal observer, estimates the marginal $\tilde{p}_\text{m}$, and the action chosen is the one that maximizes the Kullback-Leibler divergence $KL(\tilde{p}_i||\tilde{p}_\text{m})$, with $i$ in $\{0,1\}$.
The differences between experiment A and experiment B are implemented in the \emph{priors} of the ideal observer (see figure \ref{fig:data}), that is the prior of $p_0$, $p_1$ and $p_m$ based on the experiment design. In experiment A, those distributions are identically distributed over the [0.2,0.8] range, while in experiment B, the prior distribution of $p_0$ is skewed to the left, while the prior distribution of $p_1$ is skewed to the right. This single difference has for consequence that the prior of $p_0$ and $p_1$ is less informative in experiment A than in experiment B. This uninformative prior implies a longer training in experiment A than in experiment B, which conducts to place a greater emphasis on the diversity objective. As a result, action selection in experiment A tended towards highly entropic outcomes.

\begin{figure}[t!]
\includegraphics[width=.8\linewidth]{figs/cosyne_analysis.png}

\caption{{Action selection biases in models and experiments}. Left column : Experiment A (both positive and negative contingencies). Right column : Experiment B (only positive contingencies). Top row: the prior given to the observer in the two different setups. 
%MEG data (left) where participant were healthy and the task had negative causality values. SEEG data (right) from epileptic patients who had an easier task with only positive causality values. 
%The model'prior represent the difference between the setups of SEEG and MEG data. (B) 
Second row : The ratio of the action "PLAY" generated by the ideal participant, given the prior and the value of $\Delta H$. The difference is significant in Experiment A (left, $p<0.001$). %It means the model selects the action 'Play' when it is more entropic i.e its outcome is more random. 
This is not the case in Experiment B (right, $p>0.05$). Third row: behavioral data collected from participants. (Left) Participants show a significant bias toward more surprising actions (left, $p<0.01$).  (right) No significant difference in action selection. These findings match those of the model.
}
\label{fig:data}
\end{figure}

% This action selection relies on a tight balance between two opposing tendencies, namely (ii) making correct predictions, and (ii) providing sufficiently diverse data to feed the models. 
%An important confounding factor, however, is that Experiment A was conducted with healthy subjects while Experiment B was conducted with epileptic patients, although the behavioral responses were consistent in both types of subjects. 

These results finally confirm our assumptions, suggesting that human subjects may develop information-guided action selection strategies, by combining principles of Bayesian estimation \cite{knill2004bayesian} and action read-out information maximization \cite{Klyubin2005EmpowermentAU}. Furthermore, ongoing research is underway in the laboratory to investigate whether such action-selection principles can be detected in electrophysiological signals.



% Use "Eq" instead of "Equation" for equation citations.
\section*{Introduction}
Lorem ipsum dolor sit~\cite{bib1} amet, consectetur adipiscing elit. Curabitur eget porta erat. Morbi consectetur est vel gravida pretium. Suspendisse ut dui eu ante cursus gravida non sed sem. Nullam Eq~(\ref{eq:schemeP}) sapien tellus, commodo id velit id, eleifend volutpat quam. Phasellus mauris velit, dapibus finibus elementum vel, pulvinar non tellus. Nunc pellentesque pretium diam, quis maximus dolor faucibus id.~\cite{bib2} Nunc convallis sodales ante, ut ullamcorper est egestas vitae. Nam sit amet enim ultrices, ultrices elit pulvinar, volutpat risus.

\begin{eqnarray}
\label{eq:schemeP}
	\mathrm{P_Y} = \underbrace{H(Y_n) - H(Y_n|\mathbf{V}^{Y}_{n})}_{S_Y} + \underbrace{H(Y_n|\mathbf{V}^{Y}_{n})- H(Y_n|\mathbf{V}^{X,Y}_{n})}_{T_{X\rightarrow Y}},
\end{eqnarray}

\section*{Materials and methods}
\subsection*{Etiam eget sapien nibh}

% For figure citations, please use "Fig" instead of "Figure".
Nulla mi mi, Fig~\ref{fig1} venenatis sed ipsum varius, volutpat euismod diam. Proin rutrum vel massa non gravida. Quisque tempor sem et dignissim rutrum. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi at justo vitae nulla elementum commodo eu id massa. In vitae diam ac augue semper tincidunt eu ut eros. Fusce fringilla erat porttitor lectus cursus, \nameref{S1_Video} vel sagittis arcu lobortis. Aliquam in enim semper, aliquam massa id, cursus neque. Praesent faucibus semper libero.

% Place figure captions after the first paragraph in which they are cited.
\begin{figure}[!h]
\caption{{\bf Bold the figure title.}
Figure caption text here, please use this space for the figure panel descriptions instead of using subfigure commands. A: Lorem ipsum dolor sit amet. B: Consectetur adipiscing elit.}
\label{fig1}
\end{figure}

% Results and Discussion can be combined.
\section*{Results}
Nulla mi mi, venenatis sed ipsum varius, Table~\ref{table1} volutpat euismod diam. Proin rutrum vel massa non gravida. Quisque tempor sem et dignissim rutrum. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi at justo vitae nulla elementum commodo eu id massa. In vitae diam ac augue semper tincidunt eu ut eros. Fusce fringilla erat porttitor lectus cursus, vel sagittis arcu lobortis. Aliquam in enim semper, aliquam massa id, cursus neque. Praesent faucibus semper libero.

% Place tables after the first paragraph in which they are cited.
\begin{table}[!ht]
\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{
{\bf Table caption Nulla mi mi, venenatis sed ipsum varius, volutpat euismod diam.}}
\begin{tabular}{|l+l|l|l|l|l|l|l|}
\hline
\multicolumn{4}{|l|}{\bf Heading1} & \multicolumn{4}{|l|}{\bf Heading2}\\ \thickhline
$cell1 row1$ & cell2 row 1 & cell3 row 1 & cell4 row 1 & cell5 row 1 & cell6 row 1 & cell7 row 1 & cell8 row 1\\ \hline
$cell1 row2$ & cell2 row 2 & cell3 row 2 & cell4 row 2 & cell5 row 2 & cell6 row 2 & cell7 row 2 & cell8 row 2\\ \hline
$cell1 row3$ & cell2 row 3 & cell3 row 3 & cell4 row 3 & cell5 row 3 & cell6 row 3 & cell7 row 3 & cell8 row 3\\ \hline
\end{tabular}
\begin{flushleft} Table notes Phasellus venenatis, tortor nec vestibulum mattis, massa tortor interdum felis, nec pellentesque metus tortor nec nisl. Ut ornare mauris tellus, vel dapibus arcu suscipit sed.
\end{flushleft}
\label{table1}
\end{adjustwidth}
\end{table}


%PLOS does not support heading levels beyond the 3rd (no 4th level headings).
\subsection*{\lorem\ and \ipsum\ nunc blandit a tortor}
\subsubsection*{3rd level heading} 
Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque. Quisque augue sem, tincidunt sit amet feugiat eget, ullamcorper sed velit. Sed non aliquet felis. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Mauris commodo justo ac dui pretium imperdiet. Sed suscipit iaculis mi at feugiat. 

\begin{enumerate}
	\item{react}
	\item{diffuse free particles}
	\item{increment time by dt and go to 1}
\end{enumerate}


\subsection*{Sed ac quam id nisi malesuada congue}

Nulla mi mi, venenatis sed ipsum varius, volutpat euismod diam. Proin rutrum vel massa non gravida. Quisque tempor sem et dignissim rutrum. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi at justo vitae nulla elementum commodo eu id massa. In vitae diam ac augue semper tincidunt eu ut eros. Fusce fringilla erat porttitor lectus cursus, vel sagittis arcu lobortis. Aliquam in enim semper, aliquam massa id, cursus neque. Praesent faucibus semper libero.

\begin{itemize}
	\item First bulleted item.
	\item Second bulleted item.
	\item Third bulleted item.
\end{itemize}

\section*{Discussion}
Nulla mi mi, venenatis sed ipsum varius, Table~\ref{table1} volutpat euismod diam. Proin rutrum vel massa non gravida. Quisque tempor sem et dignissim rutrum. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi at justo vitae nulla elementum commodo eu id massa. In vitae diam ac augue semper tincidunt eu ut eros. Fusce fringilla erat porttitor lectus cursus, vel sagittis arcu lobortis. Aliquam in enim semper, aliquam massa id, cursus neque. Praesent faucibus semper libero~\cite{bib3}.

\section*{Conclusion}

CO\textsubscript{2} Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque. Quisque augue sem, tincidunt sit amet feugiat eget, ullamcorper sed velit. 

Sed non aliquet felis. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Mauris commodo justo ac dui pretium imperdiet. Sed suscipit iaculis mi at feugiat. Ut neque ipsum, luctus id lacus ut, laoreet scelerisque urna. Phasellus venenatis, tortor nec vestibulum mattis, massa tortor interdum felis, nec pellentesque metus tortor nec nisl. Ut ornare mauris tellus, vel dapibus arcu suscipit sed. Nam condimentum sem eget mollis euismod. Nullam dui urna, gravida venenatis dui et, tincidunt sodales ex. Nunc est dui, sodales sed mauris nec, auctor sagittis leo. Aliquam tincidunt, ex in facilisis elementum, libero lectus luctus est, non vulputate nisl augue at dolor. For more information, see \nameref{S1_Appendix}.

\section*{Supporting information}

% Include only the SI item label in the paragraph heading. Use the \nameref{label} command to cite SI items in the text.
\paragraph*{S1 Fig.}
\label{S1_Fig}
{\bf Bold the title sentence.} Add descriptive text after the title of the item (optional).

\paragraph*{S2 Fig.}
\label{S2_Fig}
{\bf Lorem ipsum.} Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.

\paragraph*{S1 File.}
\label{S1_File}
{\bf Lorem ipsum.}  Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.

\paragraph*{S1 Video.}
\label{S1_Video}
{\bf Lorem ipsum.}  Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.

\paragraph*{S1 Appendix.}
\label{S1_Appendix}
{\bf Lorem ipsum.} Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.

\paragraph*{S1 Table.}
\label{S1_Table}
{\bf Lorem ipsum.} Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.

\section*{Acknowledgments}
Cras egestas velit mauris, eu mollis turpis pellentesque sit amet. Interdum et malesuada fames ac ante ipsum primis in faucibus. Nam id pretium nisi. Sed ac quam id nisi malesuada congue. Sed interdum aliquet augue, at pellentesque quam rhoncus vitae.

\nolinenumbers

% Either type in your references using
% \begin{thebibliography}{}
% \bibitem{}
% Text
% \end{thebibliography}
%
% or
%
% Compile your BiBTeX database using our plos2015.bst
% style file and paste the contents of your .bbl file
% here. See http://journals.plos.org/plosone/s/latex for 
% step-by-step instructions.
% 

%\begin{thebibliography}{10}

%\bibitem{bib1}
%Conant GC, Wolfe KH.
%\newblock {{T}urning a hobby into a job: how duplicated genes find new
%  functions}.
%\newblock Nat Rev Genet. 2008 Dec;9(12):938--950.

%\bibitem{bib2}
%Ohno S.
%\newblock Evolution by gene duplication.
%\newblock London: George Alien \& Unwin Ltd. Berlin, Heidelberg and New York:
%  Springer-Verlag.; 1970.

%\bibitem{bib3}
%Magwire MM, Bayer F, Webster CL, Cao C, Jiggins FM.
%\newblock {{S}uccessive increases in the resistance of {D}rosophila to viral
%  infection through a transposon insertion followed by a {D}uplication}.
%\newblock PLoS Genet. 2011 Oct;7(10):e1002337.

%\end{thebibliography}

\bibliographystyle{apalike} 
%\setlength{\bibleftmargin}{.125in}
%\setlength{\bibindent}{-\bibleftmargin}

\bibliography{biblio}
%\printbibliography

\end{document}



